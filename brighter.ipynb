{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Qzj6XzxAY2pg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from transformers import AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AFRIKAANS"
      ],
      "metadata": {
        "id": "v-NqhFa5OgdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### AFRIKAANS ####\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "# Split the dataset\n",
        "splits = {'train': 'afr/train-00000-of-00001.parquet', 'dev': 'afr/dev-00000-of-00001.parquet', 'test': 'afr/test-00000-of-00001.parquet'}\n",
        "\n",
        "# Training df\n",
        "df_train_afr = pd.read_parquet(\"hf://datasets/brighter-dataset/BRIGHTER-emotion-categories/\" + splits[\"train\"])\n",
        "df_train_afr.head()\n",
        "\n",
        "# Dev df\n",
        "df_dev_afr = pd.read_parquet(\"hf://datasets/brighter-dataset/BRIGHTER-emotion-categories/\" + splits[\"dev\"])\n",
        "df_dev_afr.head()\n",
        "\n",
        "# Testing df\n",
        "df_test_afr = pd.read_parquet(\"hf://datasets/brighter-dataset/BRIGHTER-emotion-categories/\" + splits[\"test\"])\n",
        "df_test_afr.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "SIF049S9cZxM",
        "outputId": "f78540b7-9ba3-453a-bf04-d6cad9129910"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       id                                               text  \\\n",
              "0  afr_test_track_a_00001  as regering is ons daartoe verbind om ons deel...   \n",
              "1  afr_test_track_a_00002  op die oomblik is die kwessie van voedselsekur...   \n",
              "2  afr_test_track_a_00003  ek hoor dikwels mense sê hulle is gereed om be...   \n",
              "3  afr_test_track_a_00004  hiervan bly kindermishandeling waarskynlik een...   \n",
              "4  afr_test_track_a_00005  so gaan ons ernstig kyk na die kwaliteit van o...   \n",
              "\n",
              "   anger  disgust  fear  joy  sadness  surprise                   emotions  \n",
              "0      0        0     0    1        0       NaN                      [joy]  \n",
              "1      0        0     1    0        1       NaN            [fear, sadness]  \n",
              "2      0        0     0    0        0       NaN                         []  \n",
              "3      1        1     0    0        1       NaN  [anger, disgust, sadness]  \n",
              "4      0        0     0    0        0       NaN                         []  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba575fd5-b0a2-41c5-bb1c-2d04c83d86eb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>anger</th>\n",
              "      <th>disgust</th>\n",
              "      <th>fear</th>\n",
              "      <th>joy</th>\n",
              "      <th>sadness</th>\n",
              "      <th>surprise</th>\n",
              "      <th>emotions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>afr_test_track_a_00001</td>\n",
              "      <td>as regering is ons daartoe verbind om ons deel...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[joy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>afr_test_track_a_00002</td>\n",
              "      <td>op die oomblik is die kwessie van voedselsekur...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[fear, sadness]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>afr_test_track_a_00003</td>\n",
              "      <td>ek hoor dikwels mense sê hulle is gereed om be...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>afr_test_track_a_00004</td>\n",
              "      <td>hiervan bly kindermishandeling waarskynlik een...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[anger, disgust, sadness]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>afr_test_track_a_00005</td>\n",
              "      <td>so gaan ons ernstig kyk na die kwaliteit van o...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba575fd5-b0a2-41c5-bb1c-2d04c83d86eb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ba575fd5-b0a2-41c5-bb1c-2d04c83d86eb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ba575fd5-b0a2-41c5-bb1c-2d04c83d86eb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e8451259-2d54-426f-91e2-aba3193d1cfe\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e8451259-2d54-426f-91e2-aba3193d1cfe')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e8451259-2d54-426f-91e2-aba3193d1cfe button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_test_afr",
              "summary": "{\n  \"name\": \"df_test_afr\",\n  \"rows\": 2130,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2130,\n        \"samples\": [\n          \"afr_test_track_a_00283\",\n          \"afr_test_track_c_00943\",\n          \"afr_test_track_c_00648\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1065,\n        \"samples\": [\n          \"die geleentheid was die viering van die 25-jarige bestaan van die vrystaatse republiek.\",\n          \"ons vergadering op pacaltsdorp naby george was deel van die provinsiale regering se strewe na deursigtigheid en verantwoording.\",\n          \"ons wat in regering is het hierdie verantwoordelikheid aanvaar.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"anger\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disgust\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fear\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"joy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sadness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"surprise\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"emotions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### AFRIKAANS ####\n",
        "# Structure of the dataset\n",
        "print(df_train_afr.columns)\n",
        "print(df_train_afr.iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8ATv-G6DLb6",
        "outputId": "8ff20932-cf99-45dc-f318-0ebfe427402a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'text', 'anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise',\n",
            "       'emotions'],\n",
            "      dtype='object')\n",
            "id                                    afr_train_track_a_00001\n",
            "text        die grondeienaars het die departement genader ...\n",
            "anger                                                       0\n",
            "disgust                                                     0\n",
            "fear                                                        0\n",
            "joy                                                         0\n",
            "sadness                                                     0\n",
            "surprise                                                  NaN\n",
            "emotions                                                   []\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### SWAHILI ####\n",
        "# Check for empty rows\n",
        "empty_text_rows = df_train_afr['text'].str.strip().eq('')\n",
        "print(f\"Empty text rows: {empty_text_rows.sum()}\")\n",
        "\n",
        "# Check for NaN\n",
        "print(f\"NaN: {df_train_afr.isnull().sum()}\")\n",
        "\n",
        "# Fill 'surprise' column with '0'\n",
        "df_train_afr['surprise'] = df_train_afr['surprise'].fillna(0)\n",
        "df_dev_afr['surprise'] = df_dev_afr['surprise'].fillna(0)\n",
        "df_test_afr['surprise'] = df_test_afr['surprise'].fillna(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjM1uEAqPGuc",
        "outputId": "5793e85e-499e-45b6-dc7f-2a7574a874cd"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty text rows: 0\n",
            "NaN: id             0\n",
            "text           0\n",
            "anger          0\n",
            "disgust        0\n",
            "fear           0\n",
            "joy            0\n",
            "sadness        0\n",
            "surprise    1222\n",
            "emotions       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### AFRIKAANS ####\n",
        "# Remove duplicates based on 'text' column\n",
        "print(\"Duplicate texts in train:\", df_train_afr['text'].duplicated().sum())\n",
        "print(\"Duplicate texts in dev:\", df_dev_afr['text'].duplicated().sum())\n",
        "print(\"Duplicate texts in test:\", df_test_afr['text'].duplicated().sum())\n",
        "\n",
        "# Keep only first occurence of duplicate\n",
        "df_train_afr = df_train_afr.drop_duplicates(subset='text')\n",
        "df_dev_afr = df_dev_afr.drop_duplicates(subset='text')\n",
        "df_test_afr = df_test_afr.drop_duplicates(subset='text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n9dBo9hRrAb",
        "outputId": "138afe5f-3be9-49aa-e33d-36ce245d5e8c"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate texts in train: 0\n",
            "Duplicate texts in dev: 98\n",
            "Duplicate texts in test: 1065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### AFRIKAANS ####\n",
        "# Remove punctuation and spaces\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df_train_afr['text'] = df_train_afr['text'].astype(str).apply(clean_text)\n",
        "df_dev_afr['text'] = df_dev_afr['text'].astype(str).apply(clean_text)\n",
        "df_test_afr['text'] = df_test_afr['text'].astype(str).apply(clean_text)"
      ],
      "metadata": {
        "id": "2zJDrSS_K9pQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712a1ccd-2796-436e-e88b-9fdc100a9a67"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-91-2030011785>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_dev_afr['text'] = df_dev_afr['text'].astype(str).apply(clean_text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### AFRIKAANS ####\n",
        "# Convert multi-labels to binary vectors\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "def safe_eval(x):\n",
        "    try:\n",
        "        return eval(x)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "df_train_afr['labels'] = df_train_afr['emotions'].apply(safe_eval)\n",
        "df_dev_afr['labels'] = df_dev_afr['emotions'].apply(safe_eval)\n",
        "df_test_afr['labels'] = df_test_afr['emotions'].apply(safe_eval)\n",
        "\n",
        "y_train_afr = mlb.fit_transform(df_train_afr['labels'])\n",
        "y_dev_afr = mlb.transform(df_dev_afr['labels'])\n",
        "y_test_afr = mlb.transform(df_test_afr['labels'])"
      ],
      "metadata": {
        "id": "887EAj3NLCaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeafe019-fbe0-4241-a739-b94c8b421060"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-92-2122633079>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_dev_afr['labels'] = df_dev_afr['emotions'].apply(safe_eval)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### AFRIKAANS ####\n",
        "# Multi-label class names\n",
        "label_classes_afr = mlb.classes_\n",
        "print(label_classes_afr)\n",
        "\n",
        "# Tokenize with mBert\n",
        "tokenizer_afr = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "def tokenize_texts(texts):\n",
        "    return tokenizer_afr(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_encodings_afr = tokenize_texts(df_train_afr['text'].tolist())\n",
        "dev_encodings_afr = tokenize_texts(df_dev_afr['text'].tolist())\n",
        "test_encodings_afr = tokenize_texts(df_test_afr['text'].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "au5NV-AoLEVN",
        "outputId": "0f1c59c3-0c9f-4f61-a82c-e2f1117a0109"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### AFRIKAANS ####\n",
        "from datasets import Dataset\n",
        "# Convert clean and preprocessed df into Hugging face dataset format\n",
        "train_dataset_afr = Dataset.from_dict({\n",
        "    'input_ids': train_encodings_afr['input_ids'],\n",
        "    'attention_mask': train_encodings_afr['attention_mask'],\n",
        "    'labels': y_train_afr.tolist()\n",
        "})\n",
        "\n",
        "dev_dataset_afr = Dataset.from_dict({\n",
        "    'input_ids': dev_encodings_afr['input_ids'],\n",
        "    'attention_mask': dev_encodings_afr['attention_mask'],\n",
        "    'labels': y_dev_afr.tolist()\n",
        "})\n",
        "\n",
        "test_dataset_afr = Dataset.from_dict({\n",
        "    'input_ids': test_encodings_afr['input_ids'],\n",
        "    'attention_mask': test_encodings_afr['attention_mask'],\n",
        "    'labels': y_test_afr.tolist()\n",
        "})"
      ],
      "metadata": {
        "id": "zK-Rx5QaLkRO"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SWAHILI"
      ],
      "metadata": {
        "id": "79Rf5IjMOk3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### SWAHILI ####\n",
        "# Split the data\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "splits = {'train': 'swa/train-00000-of-00001.parquet', 'dev': 'swa/dev-00000-of-00001.parquet', 'test': 'swa/test-00000-of-00001.parquet'}\n",
        "\n",
        "# Training df\n",
        "df_train_swa = pd.read_parquet(\"hf://datasets/brighter-dataset/BRIGHTER-emotion-categories/\" + splits[\"train\"])\n",
        "\n",
        "# Dev df\n",
        "df_dev_swa = pd.read_parquet(\"hf://datasets/brighter-dataset/BRIGHTER-emotion-categories/\" + splits[\"dev\"])\n",
        "\n",
        "# Testing df\n",
        "df_test_swa = pd.read_parquet(\"hf://datasets/brighter-dataset/BRIGHTER-emotion-categories/\" + splits[\"test\"])\n"
      ],
      "metadata": {
        "id": "61aeWlYIfGna"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### SWAHILI ####\n",
        "# Structure of the dataset\n",
        "print(df_train_swa.columns)\n",
        "print(df_train_swa.iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq2x6exBQys-",
        "outputId": "a9f2bfe8-9949-423f-da1d-9d2a082f1b05"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'text', 'anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise',\n",
            "       'emotions'],\n",
            "      dtype='object')\n",
            "id                               swa_train_track_a_00001\n",
            "text        hii game ni goals zimetukataa tu kumamaye 😭💔\n",
            "anger                                                  1\n",
            "disgust                                                0\n",
            "fear                                                   0\n",
            "joy                                                    0\n",
            "sadness                                                0\n",
            "surprise                                               0\n",
            "emotions                                         [anger]\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### SWAHILI ####\n",
        "# Check for empty rows\n",
        "empty_text_rows = df_train_swa['text'].str.strip().eq('')\n",
        "print(f\"Empty text rows: {empty_text_rows.sum()}\")\n",
        "\n",
        "# Check for NaN\n",
        "print(f\"NaN: {df_train_swa.isnull().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2vKkOgzPLIX",
        "outputId": "4aa04a4f-a0e9-43b3-b367-efbb9d217bdd"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty text rows: 0\n",
            "NaN: id          0\n",
            "text        0\n",
            "anger       0\n",
            "disgust     0\n",
            "fear        0\n",
            "joy         0\n",
            "sadness     0\n",
            "surprise    0\n",
            "emotions    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### SWAHILI ####\n",
        "# Check for duplicate texts\n",
        "print(\"Duplicate texts in Swahili train:\", df_train_swa['text'].duplicated().sum())\n",
        "print(\"Duplicate texts in Swahili dev:\", df_dev_swa['text'].duplicated().sum())\n",
        "print(\"Duplicate texts in Swahili test:\", df_test_swa['text'].duplicated().sum())\n",
        "\n",
        "# Keep first occurence of duplicate\n",
        "df_train_swa = df_train_swa.drop_duplicates(subset='text')\n",
        "df_dev_swa = df_dev_swa.drop_duplicates(subset='text')\n",
        "df_test_swa = df_test_swa.drop_duplicates(subset='text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KpZfXojSFJ3",
        "outputId": "76bb739a-d275-437e-a17d-bc53a729276c"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate texts in Swahili train: 27\n",
            "Duplicate texts in Swahili dev: 551\n",
            "Duplicate texts in Swahili test: 1664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### SWAHILI ####\n",
        "# Remove punctuation and spaces\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df_train_swa['text'] = df_train_swa['text'].astype(str).apply(clean_text)\n",
        "df_dev_swa['text'] = df_dev_swa['text'].astype(str).apply(clean_text)\n",
        "df_test_swa['text'] = df_test_swa['text'].astype(str).apply(clean_text)"
      ],
      "metadata": {
        "id": "qxOEqa6qMWIn"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### SWAHILI ####\n",
        "# Convert multi-labels to binary vectors\n",
        "df_train_swa['labels'] = df_train_swa['emotions']\n",
        "df_dev_swa['labels'] = df_dev_swa['emotions']\n",
        "df_test_swa['labels'] = df_test_swa['emotions']\n",
        "\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train_swa = mlb.fit_transform(df_train_swa['labels'])\n",
        "y_dev_swa = mlb.transform(df_dev_swa['labels'])\n",
        "y_test_swa = mlb.transform(df_test_swa['labels'])\n",
        "\n",
        "label_classes_swa = mlb.classes_\n",
        "print(\"Swahili label classes:\", label_classes_swa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5J4GzqVMjzj",
        "outputId": "7c94f91b-4807-4a80-dd16-b4ec0e25bf2a"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Swahili label classes: ['anger' 'disgust' 'fear' 'joy' 'sadness' 'surprise']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### SWAHILI ####\n",
        "# Tokenization with mBERT\n",
        "tokenizer_swa = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "def tokenize_texts(texts):\n",
        "    return tokenizer_swa(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_encodings_swa = tokenize_texts(df_train_swa['text'].tolist())\n",
        "dev_encodings_swa = tokenize_texts(df_dev_swa['text'].tolist())\n",
        "test_encodings_swa = tokenize_texts(df_test_swa['text'].tolist())\n"
      ],
      "metadata": {
        "id": "c_hjBi1vMrm7"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### SWAHILI ####\n",
        "# Convert clean and preprocessed df into Hugging face dataset format\n",
        "train_dataset_swa = Dataset.from_dict({\n",
        "    'input_ids': train_encodings_swa['input_ids'],\n",
        "    'attention_mask': train_encodings_swa['attention_mask'],\n",
        "    'labels': y_train_swa.tolist()\n",
        "})\n",
        "\n",
        "dev_dataset_swa = Dataset.from_dict({\n",
        "    'input_ids': dev_encodings_swa['input_ids'],\n",
        "    'attention_mask': dev_encodings_swa['attention_mask'],\n",
        "    'labels': y_dev_swa.tolist()\n",
        "})\n",
        "\n",
        "test_dataset_swa = Dataset.from_dict({\n",
        "    'input_ids': test_encodings_swa['input_ids'],\n",
        "    'attention_mask': test_encodings_swa['attention_mask'],\n",
        "    'labels': y_test_swa.tolist()\n",
        "})"
      ],
      "metadata": {
        "id": "COhtIbJqNTvd"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENGLISH"
      ],
      "metadata": {
        "id": "KEnq7MxwOoLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### ENGLISH ####\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "# Split the dataset\n",
        "\n",
        "splits = {\n",
        "    'train': 'eng/train-00000-of-00001.parquet',\n",
        "    'dev': 'eng/dev-00000-of-00001.parquet',\n",
        "    'test': 'eng/test-00000-of-00001.parquet'\n",
        "}\n",
        "\n",
        "# Training df\n",
        "df_train_eng = pd.read_parquet(\"hf://datasets/brighter-dataset/BRIGHTER-emotion-categories/\" + splits[\"train\"])\n",
        "\n",
        "# Dev df\n",
        "df_dev_eng = pd.read_parquet(\"hf://datasets/brighter-dataset/BRIGHTER-emotion-categories/\" + splits[\"dev\"])\n",
        "\n",
        "#Testing df\n",
        "df_test_eng = pd.read_parquet(\"hf://datasets/brighter-dataset/BRIGHTER-emotion-categories/\" + splits[\"test\"])\n"
      ],
      "metadata": {
        "id": "1FYio-sevUgS"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### ENGLISH ####\n",
        "empty_text_rows = df_train_eng['text'].str.strip().eq('')\n",
        "print(f\"Empty text rows: {empty_text_rows.sum()}\")\n",
        "\n",
        "# Check for NaN\n",
        "print(f\"NaN: {df_train_eng.isnull().sum()}\")\n",
        "\n",
        "# Fill 'disgust' column with '0'\n",
        "df_train_eng['disgust'] = df_train_eng['disgust'].fillna(0)\n",
        "df_dev_eng['disgust'] = df_dev_eng['disgust'].fillna(0)\n",
        "df_test_eng['disgust'] = df_test_eng['disgust'].fillna(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu6ci5jbPPS-",
        "outputId": "b376a71d-6c33-4601-c6e2-fa2884858acf"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty text rows: 0\n",
            "NaN: id             0\n",
            "text           0\n",
            "anger          0\n",
            "disgust     2768\n",
            "fear           0\n",
            "joy            0\n",
            "sadness        0\n",
            "surprise       0\n",
            "emotions       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### ENGLISH ####\n",
        "# Check for duplicate texts\n",
        "print(\"Duplicate texts in English train:\", df_train_eng['text'].duplicated().sum())\n",
        "print(\"Duplicate texts in English dev:\", df_dev_eng['text'].duplicated().sum())\n",
        "print(\"Duplicate texts in English test:\", df_test_eng['text'].duplicated().sum())\n",
        "\n",
        "# Keep first occurence of duplicate\n",
        "df_train_eng = df_train_eng.drop_duplicates(subset='text')\n",
        "df_dev_eng = df_dev_eng.drop_duplicates(subset='text')\n",
        "df_test_eng = df_test_eng.drop_duplicates(subset='text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0aQ4u84SMOv",
        "outputId": "7274f29b-e8af-4ee4-bd18-5b8ac89f2ddb"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate texts in English train: 4\n",
            "Duplicate texts in English dev: 116\n",
            "Duplicate texts in English test: 2772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### ENGLISH ####\n",
        "# Remove punctuation and spaces\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df_train_eng['text'] = df_train_eng['text'].astype(str).apply(clean_text)\n",
        "df_dev_eng['text'] = df_dev_eng['text'].astype(str).apply(clean_text)\n",
        "df_test_eng['text'] = df_test_eng['text'].astype(str).apply(clean_text)"
      ],
      "metadata": {
        "id": "HuUnDXpZN1Oy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a168596-c5de-4d7d-cfcc-91d5996aeb30"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-2098075357>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_eng['text'] = df_train_eng['text'].astype(str).apply(clean_text)\n",
            "<ipython-input-106-2098075357>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_dev_eng['text'] = df_dev_eng['text'].astype(str).apply(clean_text)\n",
            "<ipython-input-106-2098075357>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_test_eng['text'] = df_test_eng['text'].astype(str).apply(clean_text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### ENGLISH ####\n",
        "# Convert multi-labels to binary vectors\n",
        "df_train_eng['labels'] = df_train_eng['emotions']\n",
        "df_dev_eng['labels'] = df_dev_eng['emotions']\n",
        "df_test_eng['labels'] = df_test_eng['emotions']\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "y_train_eng = mlb.fit_transform(df_train_eng['labels'])\n",
        "y_dev_eng = mlb.transform(df_dev_eng['labels'])\n",
        "y_test_eng = mlb.transform(df_test_eng['labels'])\n",
        "\n",
        "print(\"English label classes:\", mlb.classes_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le-PDPa-OCRM",
        "outputId": "1418acbe-5e20-4d1b-ef97-f9b6f53b45c7"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English label classes: ['anger' 'fear' 'joy' 'sadness' 'surprise']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### ENGLISH ####\n",
        "# Tokenize texts using mBERT tokenizer\n",
        "tokenizer_eng = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "def tokenize_texts(texts):\n",
        "    return tokenizer_eng(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_encodings_eng = tokenize_texts(df_train_eng['text'].tolist())\n",
        "dev_encodings_eng = tokenize_texts(df_dev_eng['text'].tolist())\n",
        "test_encodings_eng = tokenize_texts(df_test_eng['text'].tolist())"
      ],
      "metadata": {
        "id": "P7fr7J6yNiii"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### ENGLISH ####\n",
        "# Convert clean and preprocessed df into Hugging face dataset format\n",
        "train_dataset_eng = Dataset.from_dict({\n",
        "    'input_ids': train_encodings_eng['input_ids'],\n",
        "    'attention_mask': train_encodings_eng['attention_mask'],\n",
        "    'labels': y_train_eng.tolist()\n",
        "})\n",
        "\n",
        "dev_dataset_eng = Dataset.from_dict({\n",
        "    'input_ids': dev_encodings_eng['input_ids'],\n",
        "    'attention_mask': dev_encodings_eng['attention_mask'],\n",
        "    'labels': y_dev_eng.tolist()\n",
        "})\n",
        "\n",
        "test_dataset_eng = Dataset.from_dict({\n",
        "    'input_ids': test_encodings_eng['input_ids'],\n",
        "    'attention_mask': test_encodings_eng['attention_mask'],\n",
        "    'labels': y_test_eng.tolist()\n",
        "})"
      ],
      "metadata": {
        "id": "6JdC9fKtOVnd"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross-Lingual Transfer**"
      ],
      "metadata": {
        "id": "VJwzcF9CSxdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample raw emotions:\", df_train_eng['emotions'].head(10).tolist())\n",
        "print(\"Types:\", df_train_eng['emotions'].apply(type).value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc1S8Ra-t4iA",
        "outputId": "540dbef1-c476-4dd5-f976-3df144afc8b2"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample raw emotions: [array(['fear', 'surprise'], dtype=object), array(['fear'], dtype=object), array(['fear', 'sadness'], dtype=object), array([], dtype=object), array(['fear', 'sadness', 'surprise'], dtype=object), array(['fear', 'surprise'], dtype=object), array(['anger', 'fear'], dtype=object), array(['fear', 'sadness'], dtype=object), array(['fear'], dtype=object), array(['joy'], dtype=object)]\n",
            "Types: emotions\n",
            "<class 'numpy.ndarray'>    2764\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------\n",
        "# Import necessary libraries\n",
        "# -----------------------------------------\n",
        "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoTokenizer\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import ast\n",
        "\n",
        "# -----------------------------------------\n",
        "# Load and preprocess data\n",
        "# -----------------------------------------\n",
        "\n",
        "# Create an instance of MultiLabelBinarizer to one-hot encode label lists\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "# Define a safe version of eval to parse stringified lists in the 'emotions' column\n",
        "def safe_eval(x):\n",
        "    try:\n",
        "        return ast.literal_eval(x) if isinstance(x, str) else []\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "# Apply label parsing and assignment to each dataframe\n",
        "# The 'emotions' column is assumed to contain stringified lists like \"['joy', 'surprise']\"\n",
        "# This maps those lists directly into a new 'labels' column (can apply `safe_eval` here if needed)\n",
        "for df in [df_train_eng, df_dev_eng, df_test_eng,\n",
        "           df_train_afr, df_dev_afr, df_test_afr,\n",
        "           df_train_swa, df_dev_swa, df_test_swa]:\n",
        "    df['labels'] = df['emotions']  # Replace with: df['labels'] = df['emotions'].apply(safe_eval) if parsing is needed\n",
        "\n",
        "# Sanity check to confirm correct label format after assignment\n",
        "print(\"Sample parsed labels:\", df_train_eng['labels'].head())\n",
        "\n",
        "# -----------------------------------------\n",
        "# Binarize labels (one-hot encoding for multi-label classification)\n",
        "# -----------------------------------------\n",
        "\n",
        "# Fit the binarizer on English training labels only\n",
        "# This establishes the class order and builds the one-hot encoding matrix\n",
        "y_train_eng = mlb.fit_transform(df_train_eng['labels']).astype(np.float32)\n",
        "\n",
        "# Transform other sets using the same class mapping\n",
        "y_dev_eng = mlb.transform(df_dev_eng['labels']).astype(np.float32)\n",
        "y_test_eng = mlb.transform(df_test_eng['labels']).astype(np.float32)\n",
        "y_test_afr = mlb.transform(df_test_afr['labels']).astype(np.float32)\n",
        "y_test_swa = mlb.transform(df_test_swa['labels']).astype(np.float32)\n",
        "\n",
        "# Confirm output shapes and class mapping\n",
        "print(\"Classes:\", mlb.classes_)  # e.g., ['anger' 'fear' 'joy' 'sadness' 'surprise']\n",
        "print(\"y_train_eng shape:\", y_train_eng.shape)  # (num_samples, num_classes)\n",
        "\n",
        "# -----------------------------------------\n",
        "# Tokenization (text → input_ids, attention_mask, etc.)\n",
        "# -----------------------------------------\n",
        "\n",
        "# Load a pre-trained multilingual BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Tokenization function that applies truncation and padding to 128 tokens\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Apply tokenization to each language-specific dataset\n",
        "train_encodings = tokenize_function(df_train_eng[\"text\"].tolist())\n",
        "dev_encodings = tokenize_function(df_dev_eng[\"text\"].tolist())\n",
        "test_encodings_eng = tokenize_function(df_test_eng[\"text\"].tolist())\n",
        "test_encodings_afr = tokenize_function(df_test_afr[\"text\"].tolist())\n",
        "test_encodings_swa = tokenize_function(df_test_swa[\"text\"].tolist())\n",
        "\n",
        "# -----------------------------------------\n",
        "# Create a PyTorch Dataset class to use with the Trainer\n",
        "# -----------------------------------------\n",
        "\n",
        "class EmotionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)  # Ensure float for multi-label BCE loss\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Gather individual sample's encoded input\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]  # Add the corresponding one-hot encoded label\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Instantiate datasets\n",
        "train_dataset_eng = EmotionDataset(train_encodings, y_train_eng)\n",
        "dev_dataset_eng = EmotionDataset(dev_encodings, y_dev_eng)\n",
        "test_dataset_eng = EmotionDataset(test_encodings_eng, y_test_eng)\n",
        "test_dataset_afr = EmotionDataset(test_encodings_afr, y_test_afr)\n",
        "test_dataset_swa = EmotionDataset(test_encodings_swa, y_test_swa)\n",
        "\n",
        "# -----------------------------------------\n",
        "# Load the model for sequence classification\n",
        "# -----------------------------------------\n",
        "\n",
        "# Load a multilingual BERT model with:\n",
        "# - Correct number of output labels\n",
        "# - Classification type explicitly set to multi-label\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=len(mlb.classes_),               # Number of output nodes = number of emotion classes\n",
        "    problem_type=\"multi_label_classification\"   # Important for correct loss function (BCEWithLogits)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtk5mclGTBdF",
        "outputId": "d0e1e3fe-d5fc-4a46-cfb6-a0411f300165"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample parsed labels: 0             [fear, surprise]\n",
            "1                       [fear]\n",
            "2              [fear, sadness]\n",
            "3                           []\n",
            "4    [fear, sadness, surprise]\n",
            "Name: labels, dtype: object\n",
            "Classes: ['anger' 'fear' 'joy' 'sadness' 'surprise']\n",
            "y_train_eng shape: (2764, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['disgust'] will be ignored\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#====TESTING====\n",
        "\n",
        "\n",
        "# Print label-related info\n",
        "print(\"Classes:\", mlb.classes_)\n",
        "print(\"Shape of y_train_eng:\", y_train_eng.shape)\n",
        "print(\"Sample binarized labels:\", y_train_eng[:5])\n",
        "print(\"Sample original labels:\", df_train_eng['labels'].head(5).tolist())\n",
        "\n",
        "# Print tokenization info\n",
        "print(\"Sample tokenized input keys:\", train_encodings.keys())\n",
        "print(\"Example tokenized input IDs for first sample:\", train_encodings['input_ids'][0])\n",
        "print(\"Length of first tokenized input:\", len(train_encodings['input_ids'][0]))\n",
        "\n",
        "# Check one item from the dataset\n",
        "sample_item = train_dataset_eng[0]\n",
        "print(\"Sample dataset item keys:\", sample_item.keys())\n",
        "print(\"Labels tensor shape:\", sample_item['labels'].shape)\n",
        "print(\"Labels tensor:\", sample_item['labels'])\n",
        "\n",
        "# Prepare a batch for testing the model\n",
        "batch = {\n",
        "    key: torch.tensor(val[:2]) for key, val in train_encodings.items()\n",
        "}\n",
        "\n",
        "# Put model in evaluation mode and run inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=batch['input_ids'],\n",
        "        attention_mask=batch['attention_mask'],\n",
        "        token_type_ids=batch.get('token_type_ids')  # safely include if exists\n",
        "    )\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Print logits info\n",
        "print(\"Output logits shape:\", logits.shape)\n",
        "print(\"Output logits (sample):\", logits)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZzXFL5ZsQBt",
        "outputId": "6594eed2-898b-4940-8222-4e8825eb6a73"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['anger' 'fear' 'joy' 'sadness' 'surprise']\n",
            "Shape of y_train_eng: (2764, 5)\n",
            "Sample binarized labels: [[0. 1. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 1. 1.]]\n",
            "Sample original labels: [array(['fear', 'surprise'], dtype=object), array(['fear'], dtype=object), array(['fear', 'sadness'], dtype=object), array([], dtype=object), array(['fear', 'sadness', 'surprise'], dtype=object)]\n",
            "Sample tokenized input keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
            "Example tokenized input IDs for first sample: [101, 14136, 11272, 20181, 10108, 11858, 57204, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Length of first tokenized input: 128\n",
            "Sample dataset item keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
            "Labels tensor shape: torch.Size([5])\n",
            "Labels tensor: tensor([0., 1., 0., 0., 1.])\n",
            "Output logits shape: torch.Size([2, 5])\n",
            "Output logits (sample): tensor([[ 0.0393, -0.3265,  0.3657, -0.1016,  0.0839],\n",
            "        [ 0.0976, -0.2612,  0.2697, -0.1293,  0.0749]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adapter-Based Fine-Tuning"
      ],
      "metadata": {
        "id": "oCoZCQM1Vlol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from adapters import ConfigUnion, PrefixTuningConfig, ParBnConfig, AutoAdapterModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Example label list\n",
        "labels = [\"joy\", \"anger\", \"sadness\", \"surprise\", \"fear\"]\n",
        "label2id = {l: i for i, l in enumerate(labels)}\n",
        "id2label = {i: l for i, l in enumerate(labels)}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Use AutoAdapterModel (instead of AutoModelForSequenceClassification) to support adapters\n",
        "model = AutoAdapterModel.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=len(labels),\n",
        "    problem_type=\"multi_label_classification\",\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# Combine adapter configs with ConfigUnion\n",
        "adapter_config = ConfigUnion(\n",
        "    PrefixTuningConfig(prefix_length=20),\n",
        "    ParBnConfig(reduction_factor=4),\n",
        ")\n",
        "\n",
        "# Add adapter with combined config and activate immediately\n",
        "model.add_adapter(\"my_adapter\", config=adapter_config, set_active=True)\n",
        "\n",
        "# Put model into adapter training mode for the adapter\n",
        "model.train_adapter(\"my_adapter\")\n",
        "\n",
        "# Optionally explicitly set active adapters (not necessary if set_active=True above)\n",
        "model.set_active_adapters(\"my_adapter\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZUE1dgNV2-k",
        "outputId": "8d8feed5-03fd-43a3-d8b3-24ce2cc69436"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['heads.default.3.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:adapters.model_mixin:There are adapters available but none are activated for the forward pass.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from adapters import ConfigUnion, PrefixTuningConfig, ParBnConfig, AutoAdapterModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# === Setup ===\n",
        "labels = [\"joy\", \"anger\", \"sadness\", \"surprise\", \"fear\"]\n",
        "label2id = {l: i for i, l in enumerate(labels)}\n",
        "id2label = {i: l for i, l in enumerate(labels)}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "model = AutoAdapterModel.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=len(labels),\n",
        "    problem_type=\"multi_label_classification\",\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "adapter_config = ConfigUnion(\n",
        "    PrefixTuningConfig(prefix_length=20),\n",
        "    ParBnConfig(reduction_factor=4),\n",
        ")\n",
        "\n",
        "model.add_adapter(\"my_adapter\", config=adapter_config, set_active=True)\n",
        "model.train_adapter(\"my_adapter\")\n",
        "model.set_active_adapters(\"my_adapter\")\n",
        "\n",
        "# === Verification ===\n",
        "print(\"\\n=== VERIFICATION CHECKS ===\")\n",
        "\n",
        "# Check if adapter config is attached properly\n",
        "has_adapters = hasattr(model.config, \"adapters\")\n",
        "print(\"Config has adapters attribute:\", has_adapters)\n",
        "\n",
        "if has_adapters:\n",
        "    print(\"Available adapters:\", model.config.adapters.adapter_list())\n",
        "else:\n",
        "    print(\"No adapters found in config!\")\n",
        "\n",
        "print(\"Active adapters:\", model.active_adapters)\n",
        "\n",
        "# Check which parameters are trainable (should be adapter params)\n",
        "trainable_params = [name for name, param in model.named_parameters() if \"adapters.my_adapter\" in name and param.requires_grad]\n",
        "print(f\"\\nTrainable adapter parameters ({len(trainable_params)}):\")\n",
        "for name in trainable_params:\n",
        "    print(\"  -\", name)\n",
        "\n",
        "# Check frozen base model parameters\n",
        "frozen_params = [name for name, param in model.named_parameters() if \"adapters.my_adapter\" not in name and not param.requires_grad]\n",
        "print(f\"\\nNumber of frozen base parameters: {len(frozen_params)}\")\n",
        "\n",
        "# Check classifier head info\n",
        "print(\"\\nClassifier head info:\")\n",
        "print(\"  - problem_type:\", model.config.problem_type)\n",
        "print(\"  - label2id:\", model.config.label2id)\n",
        "\n",
        "# Run a sample forward pass\n",
        "sample_text = \"I am feeling joyful and surprised today!\"\n",
        "inputs = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "try:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    print(\"\\nForward pass successful!\")\n",
        "    print(\"Logits shape:\", outputs.logits.shape)\n",
        "    print(\"Logits:\", outputs.logits)\n",
        "except Exception as e:\n",
        "    print(\"Forward pass failed:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAk_HRXIyVjd",
        "outputId": "47cc4bfc-6fe9-4f4d-85cb-a9c523d65b4a"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['heads.default.3.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:adapters.model_mixin:There are adapters available but none are activated for the forward pass.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== VERIFICATION CHECKS ===\n",
            "Config has adapters attribute: False\n",
            "No adapters found in config!\n",
            "Active adapters: Stack[my_adapter]\n",
            "\n",
            "Trainable adapter parameters (48):\n",
            "  - bert.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.0.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.0.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.0.output.adapters.my_adapter.adapter_up.bias\n",
            "  - bert.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.1.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.1.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.1.output.adapters.my_adapter.adapter_up.bias\n",
            "  - bert.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.2.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.2.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.2.output.adapters.my_adapter.adapter_up.bias\n",
            "  - bert.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.3.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.3.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.3.output.adapters.my_adapter.adapter_up.bias\n",
            "  - bert.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.4.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.4.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.4.output.adapters.my_adapter.adapter_up.bias\n",
            "  - bert.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.5.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.5.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.5.output.adapters.my_adapter.adapter_up.bias\n",
            "  - bert.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.6.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.6.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.6.output.adapters.my_adapter.adapter_up.bias\n",
            "  - bert.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.7.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.7.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.7.output.adapters.my_adapter.adapter_up.bias\n",
            "  - bert.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.8.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.8.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.8.output.adapters.my_adapter.adapter_up.bias\n",
            "  - bert.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.9.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.9.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.9.output.adapters.my_adapter.adapter_up.bias\n",
            "  - bert.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.10.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.10.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.10.output.adapters.my_adapter.adapter_up.bias\n",
            "  - bert.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.weight\n",
            "  - bert.encoder.layer.11.output.adapters.my_adapter.adapter_down.0.bias\n",
            "  - bert.encoder.layer.11.output.adapters.my_adapter.adapter_up.weight\n",
            "  - bert.encoder.layer.11.output.adapters.my_adapter.adapter_up.bias\n",
            "\n",
            "Number of frozen base parameters: 200\n",
            "\n",
            "Classifier head info:\n",
            "  - problem_type: multi_label_classification\n",
            "  - label2id: None\n",
            "\n",
            "Forward pass successful!\n",
            "Logits shape: torch.Size([1, 13, 119547])\n",
            "Logits: tensor([[[ -8.1116,  -8.0856,  -8.1586,  ...,  -7.8201,  -7.7018,  -7.8503],\n",
            "         [ -9.5159, -10.4066, -10.5248,  ...,  -9.5674,  -8.5997,  -9.1560],\n",
            "         [-12.9718, -14.3651, -14.0995,  ..., -11.9172, -10.6641, -11.3617],\n",
            "         ...,\n",
            "         [ -9.3941, -10.6213, -10.4149,  ..., -10.9812,  -6.9039, -10.7847],\n",
            "         [-10.3250, -10.9364,  -9.8952,  ..., -10.0307,  -7.8480, -10.6691],\n",
            "         [ -9.6005,  -9.7332,  -9.5159,  ...,  -9.0212,  -7.9403,  -9.4260]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation (Back-Translation):"
      ],
      "metadata": {
        "id": "EMOR88IQ9ATr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH9jthRo9Mwd",
        "outputId": "f8102310-4b60-4af3-82d0-30c49127cf76"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, random\n",
        "from datetime import timedelta\n",
        "\n",
        "# Sample 30% of the English training data\n",
        "num_augment = int(0.3 * len(df_train_eng))\n",
        "sampled_indices = random.sample(range(len(df_train_eng)), num_augment)\n",
        "\n",
        "augmented_texts = []\n",
        "augmented_labels = []\n",
        "\n",
        "print(f\"Generating {num_augment} augmented samples...\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "for i, idx in enumerate(sampled_indices, 1):  # Start index at 1 for nicer output\n",
        "    orig_text = df_train_eng.iloc[idx]['text']\n",
        "    back_text = back_translate(orig_text, src='en', mid='sw')\n",
        "    augmented_texts.append(back_text)\n",
        "    augmented_labels.append(df_train_eng.iloc[idx]['labels'])\n",
        "\n",
        "    # Optional delay to avoid rate limiting\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    # Show progress every 50 samples\n",
        "    if i % 50 == 0 or i == num_augment:\n",
        "        elapsed = time.time() - start_time\n",
        "        avg_time = elapsed / i\n",
        "        eta = avg_time * (num_augment - i)\n",
        "        print(f\"[{i}/{num_augment}] - Elapsed: {timedelta(seconds=int(elapsed))} - ETA: {timedelta(seconds=int(eta))}\")\n",
        "\n",
        "print(\"\\nAugmentation completed!\")\n",
        "\n",
        "\n",
        "# Create augmented DataFrame\n",
        "df_aug = pd.DataFrame({\n",
        "    \"text\": augmented_texts,\n",
        "    \"labels\": augmented_labels\n",
        "})\n",
        "\n",
        "# Combine with original data\n",
        "df_train_augmented = pd.concat([df_train_eng, df_aug], ignore_index=True)\n",
        "\n",
        "# Re-tokenize and encode\n",
        "train_encodings_aug = tokenize_function(df_train_augmented[\"text\"].tolist())\n",
        "y_train_aug = mlb.transform(df_train_augmented['labels']).astype(np.float32)\n",
        "train_dataset_aug = EmotionDataset(train_encodings_aug, y_train_aug)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPnfoc6dWAB_",
        "outputId": "0d51709b-c548-4a1f-f355-731adc894da0"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 829 augmented samples...\n",
            "\n",
            "[50/829] - Elapsed: 0:02:20 - ETA: 0:36:33\n",
            "[100/829] - Elapsed: 0:04:45 - ETA: 0:34:37\n",
            "[150/829] - Elapsed: 0:07:13 - ETA: 0:32:40\n",
            "[200/829] - Elapsed: 0:09:37 - ETA: 0:30:15\n",
            "[250/829] - Elapsed: 0:12:11 - ETA: 0:28:15\n",
            "[300/829] - Elapsed: 0:14:30 - ETA: 0:25:34\n",
            "[350/829] - Elapsed: 0:17:10 - ETA: 0:23:30\n",
            "[400/829] - Elapsed: 0:19:44 - ETA: 0:21:10\n",
            "[450/829] - Elapsed: 0:21:56 - ETA: 0:18:28\n",
            "[500/829] - Elapsed: 0:24:29 - ETA: 0:16:06\n",
            "[550/829] - Elapsed: 0:26:45 - ETA: 0:13:34\n",
            "[600/829] - Elapsed: 0:29:00 - ETA: 0:11:04\n",
            "[650/829] - Elapsed: 0:31:23 - ETA: 0:08:38\n",
            "[700/829] - Elapsed: 0:33:37 - ETA: 0:06:11\n",
            "[750/829] - Elapsed: 0:36:11 - ETA: 0:03:48\n",
            "[800/829] - Elapsed: 0:38:52 - ETA: 0:01:24\n",
            "[829/829] - Elapsed: 0:40:10 - ETA: 0:00:00\n",
            "\n",
            "Augmentation completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original training samples: {len(df_train_eng)}\")\n",
        "print(f\"Augmented samples generated: {len(df_aug)}\")\n",
        "print(f\"Total training samples after augmentation: {len(df_train_augmented)}\")\n",
        "\n",
        "\n",
        "print(\"\\nSample augmented texts and labels:\")\n",
        "print(df_aug.head())\n",
        "\n",
        "print(\"\\nSample combined training data:\")\n",
        "print(df_train_augmented.sample(5))\n",
        "\n",
        "\n",
        "print(f\"Encoded input keys: {train_encodings_aug.keys()}\")\n",
        "print(f\"Number of encoded samples: {len(next(iter(train_encodings_aug.values())))}\")\n",
        "\n",
        "print(f\"Labels shape: {y_train_aug.shape}\")\n",
        "print(f\"First label vector example: {y_train_aug[0]}\")\n",
        "\n",
        "print(f\"Dataset length: {len(train_dataset_aug)}\")\n",
        "sample_item = train_dataset_aug[0]\n",
        "print(f\"Sample dataset item keys: {sample_item.keys()}\")\n",
        "print(f\"Sample labels tensor: {sample_item['labels']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTK12-3AqCvY",
        "outputId": "0aab1ece-6966-4fd9-b3f5-a3a9b51c0077"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training samples: 2764\n",
            "Augmented samples generated: 829\n",
            "Total training samples after augmentation: 3593\n",
            "\n",
            "Sample augmented texts and labels:\n",
            "                                                text                 labels\n",
            "0                      Was a bad feeling of darkness        [fear, sadness]\n",
            "1  At the beginning was the same when I let my sl...                  [joy]\n",
            "2  I'm glad that someone was there but then again...   [fear, joy, sadness]\n",
            "3                     yikes no pun intended or count  [fear, joy, surprise]\n",
            "4  Initially sent on September 18, 2007 I walked ...        [fear, sadness]\n",
            "\n",
            "Sample combined training data:\n",
            "                           id  \\\n",
            "3340                      NaN   \n",
            "761   eng_train_track_a_00762   \n",
            "1276  eng_train_track_a_01277   \n",
            "647   eng_train_track_a_00648   \n",
            "1617  eng_train_track_a_01619   \n",
            "\n",
            "                                                   text  anger  disgust  fear  \\\n",
            "3340  Enthusiastic and filled with Hope I went into ...    NaN      NaN   NaN   \n",
            "761   so when i caught this songkahin tho hogilast e...    0.0      0.0   0.0   \n",
            "1276  as soon as i came out of the loo to wash my ha...    0.0      0.0   1.0   \n",
            "647         wishing i had triple a i stick out my thumb    0.0      0.0   0.0   \n",
            "1617  i began to feel the alltoofamiliar feeling tha...    0.0      0.0   1.0   \n",
            "\n",
            "      joy  sadness  surprise         emotions           labels  \n",
            "3340  NaN      NaN       NaN              NaN  [joy, surprise]  \n",
            "761   1.0      0.0       0.0            [joy]            [joy]  \n",
            "1276  0.0      0.0       0.0           [fear]           [fear]  \n",
            "647   0.0      0.0       0.0               []               []  \n",
            "1617  0.0      1.0       0.0  [fear, sadness]  [fear, sadness]  \n",
            "Encoded input keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
            "Number of encoded samples: 3593\n",
            "Labels shape: (3593, 5)\n",
            "First label vector example: [0. 1. 0. 0. 1.]\n",
            "Dataset length: 3593\n",
            "Sample dataset item keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
            "Sample labels tensor: tensor([0., 1., 0., 0., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Training"
      ],
      "metadata": {
        "id": "_ooRTZEecd-9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boGqEaUH-sNn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}